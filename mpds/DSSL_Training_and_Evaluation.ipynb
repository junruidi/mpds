{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import rpy2.robjects.numpy2ri\n",
    "import rpy2.robjects.pandas2ri\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rpy2.robjects.numpy2ri.activate()\n",
    "rpy2.robjects.pandas2ri.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "setwd(\"..\")\n",
    "source(\"dssFunctionLibrary.R\")\n",
    "source(\"dssDataSetSpecificHacks.R\")\n",
    "source(\"dssPerformanceEvaluation.R\")\n",
    "source(\"mimicUsefulFunction.R\")\n",
    "require(gridExtra)\n",
    "require(ROCR)\n",
    "require(ISLR)\n",
    "require(caret)\n",
    "require(magrittr)\n",
    "library(foreach)\n",
    "library(doMC)\n",
    "registerDoMC(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(os.path.join(\"\", \"passiveTrend_30s.csv\"))\n",
    "cols = dataset.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "dataset = dataset[cols]\n",
    "dataset['tsp'] = dataset['tsp'].values.astype('datetime64[s]')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids_all = dataset.id.value_counts()\n",
    "ids = ids_all.index.values\n",
    "dataset = dataset[dataset.id.isin(ids)]\n",
    "dataset_before = dataset.iloc[::2]\n",
    "dataset_after = dataset.iloc[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# play with range for lam0 and sweep to choose right regularization penalty\n",
    "LO = [1e7, 1e6, 1e5, 1e4, 1e3, 1e2, 1e1, 1, 1e-1, 1e-2] \n",
    "# play with range for lambda-S conditioned on choice for lam0\n",
    "LS = [1e7, 1e6, 1e5, 1e4, 1e3, 1e2, 1e1, 1, 1e-1, 1e-2]\n",
    "lo = []\n",
    "ls = []\n",
    "for i in LO:\n",
    "    for j in LS:\n",
    "        lo.append(i)\n",
    "        ls.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model_in_R():\n",
    "    %R -i XTrainScaled,XTrainTimeVector,lo,ls,sm_on_idx,sm_off_idx\n",
    "    %R lo <- as.numeric(unlist(lo))\n",
    "    %R ls <- as.numeric(unlist(ls))\n",
    "    %R OrderPairs <- data.frame(onIdx = seq(1, nrow(XTrainScaled)/2, by = 1),\\\n",
    "                                offIdx = seq(nrow(XTrainScaled)/2+1, nrow(XTrainScaled), by = 1))\n",
    "    %R SmoothnessPairs <- data.frame(onIdx = sm_on_idx, offIdx = sm_off_idx)\n",
    "    %R -o res res <- dssTrain.Linear(XTrainScaled, OrderPairs, SmoothnessPairs,\\\n",
    "                                     XTrainTimeVector, lo, ls,\"dssl_train\", doParallel = 1)\n",
    "    return res\n",
    "\n",
    "def get_accuracy(scores):\n",
    "    length = int(len(scores)/2)\n",
    "    diff = scores[:length] - scores[length:]\n",
    "    accuracy = np.sum(diff >= 0) * 1.0 / len(diff)\n",
    "    return accuracy\n",
    "\n",
    "def evaluate(pairs, weights):\n",
    "    scores = pairs.dot(weights)\n",
    "    return {'scores':scores, 'accuracy':get_accuracy(scores)}\n",
    "\n",
    "def weight_analysis(models, names):\n",
    "    for i, m in enumerate(models):\n",
    "        if i == 0:\n",
    "            weights = np.asarray(m['weights'])\n",
    "        else:\n",
    "            mweights = np.asarray(m['weights'])\n",
    "            weights += mweights\n",
    "    weights /= len(models)\n",
    "    df = pd.DataFrame(weights, index=names, columns=['weight'])\n",
    "    ii = df.weight.abs().sort_values(ascending=False)\n",
    "    df[\"scaled\"] = df.weight / max(df.weight.abs())\n",
    "    return df\n",
    "\n",
    "def get_fold_test_scores(test_pt, test_scores):\n",
    "    # get idx: id and tsp\n",
    "    df = dataset_before[dataset_before.id.isin(test_pt)][['id','tsp']]\\\n",
    "        .append(dataset_after[dataset_after.id.isin(test_pt)][['id','tsp']])\n",
    "    # get score columns\n",
    "    for item in test_scores:\n",
    "        if item.startswith('scores'):\n",
    "            df[item] = test_scores[item]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pts = np.unique(dataset_before.id)\n",
    "pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine select weights and lambda together \n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "fold_accuracy = []\n",
    "fold_weights = []\n",
    "fold_train_pt = []\n",
    "fold_test_pt = []\n",
    "fold_scaler = []\n",
    "fold_res_train = []\n",
    "fold_res_test = []\n",
    "\n",
    "index_cols = ['id', 'tsp']\n",
    "for train, test in kf.split(pts):\n",
    "\n",
    "    train_pt = pts[train]\n",
    "    test_pt = pts[test]\n",
    "    \n",
    "    # set aside training set\n",
    "    dataset_train = dataset_before[dataset_before.id.isin(train_pt)].\\\n",
    "        append(dataset_after[dataset_after.id.isin(train_pt)])\n",
    "    XTrain = dataset_train.drop(index_cols, axis=1)\n",
    "    XTrainTsp = pd.to_datetime(dataset_train.tsp, format='%Y-%m-%d %H:%M:%S')\n",
    "    XTrainTimeVector = (XTrainTsp - np.datetime64('1970-01-01 00:00:00')) / np.timedelta64(1, 's')\n",
    "    XTrainTimeVector = XTrainTimeVector.astype(int)\n",
    "    scaler = RobustScaler()\n",
    "    XTrainScaled = scaler.fit_transform(XTrain)\n",
    "    train_ids = dataset_train[['id','tsp']].sort_values(by=['id','tsp']).id.values\n",
    "    train_ids_idx = dataset_train[['id','tsp']].sort_values(by=['id','tsp']).index.values\n",
    "    sequential_idx_1 = train_ids[:-1] == train_ids[1:]\n",
    "    sequential_idx_2 = np.insert(sequential_idx_1, 0, False)\n",
    "    sequential_idx_1 = np.append(sequential_idx_1, False)\n",
    "    sm_on_idx = []\n",
    "    sm_off_idx = []\n",
    "\n",
    "    for i, idx_1 in enumerate(train_ids_idx[sequential_idx_1]):\n",
    "        idx_2 = train_ids_idx[sequential_idx_2][i]\n",
    "        row_i_1 = dataset_train.index.get_loc(idx_1)\n",
    "        row_i_2 = dataset_train.index.get_loc(idx_2)\n",
    "        sm_on_idx.append(row_i_2)\n",
    "        sm_off_idx.append(row_i_1)\n",
    "\n",
    "    sm_on_idx = np.add(sm_on_idx,1)\n",
    "    sm_off_idx = np.add(sm_off_idx,1)\n",
    "    \n",
    "    ### Call Learning Procedure ###\n",
    "    res = train_model_in_R()\n",
    "    print(\"split:\", train_pt, test_pt)\n",
    "    \n",
    "    # test dataset\n",
    "    dataset_test = dataset_before[dataset_before.id.isin(test_pt)]\\\n",
    "        .append(dataset_after[dataset_after.id.isin(test_pt)])\n",
    "    XTest = dataset_test.drop(index_cols, axis=1)\n",
    "    XTestScaled = scaler.transform(XTest)        \n",
    "\n",
    "    acc = []\n",
    "    weights = []\n",
    "    res_train_list = []\n",
    "    res_test_list = []\n",
    "    for i,la in enumerate(lo):\n",
    "        %R -i i -o w w = res[[i+1]]$estimate\n",
    "        res_train = evaluate(XTrainScaled, w)\n",
    "        res_test = evaluate(XTestScaled, w)\n",
    "        acc.append([(i+1), lo[i], ls[i], res_train['accuracy'], res_test['accuracy']])\n",
    "        weights.append(w)\n",
    "        res_train_list.append(res_train)\n",
    "        res_test_list.append(res_test)\n",
    "    fold_accuracy.append(acc)\n",
    "    fold_weights.append(weights)\n",
    "    fold_train_pt.append(train_pt)\n",
    "    fold_test_pt.append(test_pt)\n",
    "    fold_scaler.append(scaler)\n",
    "    fold_res_train.append(res_train_list)\n",
    "    fold_res_test.append(res_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select best hyper parameters from validation set and apply to test set\n",
    "fold_models = []\n",
    "best_acc = []\n",
    "for i, acc in enumerate(fold_accuracy):\n",
    "    model = {}\n",
    "    train_acc = [val[3] for val in acc]\n",
    "    test_acc = [val[4] for val in acc]\n",
    "    best_wi = train_acc.index(max(train_acc))\n",
    "    model['weights'] = fold_weights[i][best_wi]\n",
    "    model['scaler'] = fold_scaler[i]\n",
    "    model['wi'] = best_wi\n",
    "    fold_models.append(model)\n",
    "    print(best_wi, train_acc[best_wi], test_acc[best_wi], max(test_acc))\n",
    "    best_acc.append(test_acc[best_wi])\n",
    "np.mean(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, model in enumerate(fold_models):\n",
    "    wi = model['wi']\n",
    "    print(i, wi)\n",
    "    if i == 0:\n",
    "        scores = get_fold_test_scores(fold_test_pt[i], fold_res_test[i][wi])\n",
    "    else:\n",
    "        scores = scores.append(get_fold_test_scores(fold_test_pt[i], fold_res_test[i][wi]))\n",
    "\n",
    "scores.tsp = pd.to_datetime(scores.tsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = weight_analysis(fold_models, XTrain.columns.values)\n",
    "df[\"abs_w\"] = df.weight.abs()\n",
    "df[\"abs_w_scale\"] = 100*(df.abs_w-df.abs_w.min())/df.abs_w.max() - df.abs_w.min()\n",
    "df.abs_w_scale.sort_values(ascending=False).to_csv('sorted_weight_060118.csv',index=True)\n",
    "df.abs_w_scale[df.abs_w_scale > 30].sort_values(ascending=False)\n",
    "df.abs_w_scale[df.abs_w_scale < 1].count()\n",
    "df.abs_w_scale[df.abs_w_scale < 1].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_before = pd.DataFrame()\n",
    "for id in scores.id.unique():\n",
    "    pt_scores = scores[scores.id == id]\n",
    "    l = int(len(pt_scores)/2)\n",
    "    scores_before = scores_before.append(pt_scores[:l])   \n",
    "scores.to_csv('pt_scores_060118.csv', index=False)\n",
    "scores_before.to_csv('pt_scores_before_060118.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaled_scores = scores.copy()\n",
    "scaled_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler100 = RobustScaler(quantile_range=(15.0, 85.0))\n",
    "scaler100.fit(scaled_scores.scores.values.reshape(-1, 1))\n",
    "scaled_scores['scores_scaled'] = 50*scaler100.transform(scaled_scores.scores.values.reshape(-1, 1)) + 50\n",
    "scaled_scores.scores_scaled[(scaled_scores.scores_scaled > 100)] = 100\n",
    "scaled_scores.scores_scaled[(scaled_scores.scores_scaled < 0)] = 0\n",
    "print(scaled_scores.scores_scaled.mean(), scaled_scores.scores_scaled.std() \\\n",
    "    , scaled_scores.scores_scaled.quantile(.25), scaled_scores.scores_scaled.quantile(.75))\n",
    "scaled_scores.scores_scaled = scaled_scores.scores_scaled.round()\n",
    "scaled_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaled_scores = pd.merge(dataset, scaled_scores,how='left')\n",
    "scaled_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_scores.to_csv('scores_scaled_060318.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
